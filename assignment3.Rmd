---
title: "Assignment3"
author: "Ashutosh Mudgal"
date: "16 February 2017"
output: html_document
---
## 1. Install the package mlbench where you have the BreastCancer.rda file in the data folder.
1. KNN
2. CTREE
3.RPART
4.RandomForest
5.NaiveBayes
6. Logistic Regression
Run the above classification algorithms with the same data split and compare the Accuracy from the Confusion Matrix.

```{r}
library(mlbench)
data("BreastCancer")
View(BreastCancer)
str(BreastCancer)
BreastCancer<-BreastCancer[,-1]
library(VIM)
library(caret)
library(colorspace)
library(grid)
library(data.table)
library(Rcpp)
library(mice)
library(party)

```

##logistic regression
check and finding missing value
```{r}
imputation_plot <- aggr(BreastCancer,col = c("yellow","blue"),
                        numbers = TRUE,sortVars = TRUE,
                        labels = names(BreastCancer),gap = 3,
                        ylab = c("Missing Data","Pattern"))
imputed_Data <- mice(BreastCancer, m=5, maxit = 50, method = 'cart', seed = 500)
completeData <- complete(imputed_Data,4)

```
##impute missing values
```{r}
imputation_plot2 <- aggr(completeData,col = c("yellow","blue"),
                         numbers = TRUE,sortVars = TRUE,
                         labels = names(completeData),gap = 3,
                         ylab = c("Missing Data","Pattern"))
```

##splitting data
```{r}
set.seed(100)
trainingRowindex<-sample(1:nrow(completeData),0.78*nrow(completeData))
trainingData<-completeData[trainingRowindex,]
testData<-completeData[-trainingRowindex,]
testData

```
## regression model
```{r}
lmMod<-glm(Class ~.,data=trainingData,family = "binomial")
distPred<-predict(lmMod,testData)
summary(lmMod)
attach(trainingData)
testData$Class <-factor(testData$Class)
levels(testData$Class)<-levels(testData$Class)
testData$Dataoutput<-predict(lmMod,testData,type="response")
```

##accuracy of regression model
```{r}
table(round(testData$Dataoutput),testData$Class)
sum(diag(table(round(testData$Dataoutput),testData$Class)))/sum(table(round(testData$Dataoutput),testData$Class))

```

##CTREE
```{r}
output.tree<-ctree(Class~.,data=testData,
                   controls = ctree_control(maxdepth = 3))

```
##plot the tree
```{r}

plot(output.tree)
testData$output<-predict(output.tree,testData)
```
##CTREE Accuracy
```{r}
table((testData$output),testData$Class)
sum(diag(table((testData$output),testData$Class)))/sum(table((testData$output),testData$Class))
```

##RPART


```{r}
library(rpart)
fit<-rpart(Class~.,method = "class",data =trainingData)
plot(fit)
fit
testData$output<-predict(fit,testData,type="class")
confusion=table((testData$output),testData$Class)
confusion
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
```

##RANDOM FOREST

```{r}
library(randomForest)
fit<-randomForest(Class ~.,method = "class",data=trainingData)
output.forest<-randomForest(Class~.,data=trainingData,ntree=1600)
output.forest
testData$output.forest<-predict(fit,testData,type = "class")
testData$output.forest
confusion<-table(testData$output.forest,testData$Class)
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy
```

## Naive Bayes

```{r}
library(e1071)
modelNB<- naiveBayes(Class ~.,data = trainingData)
modelNB
pred<-predict(modelNB,testData)
pred
confusion<-table(pred,testData$Class)
accuracy<-sum(diag(confusion))/sum(confusion)
accuracy

```
## KNN

```{r}
library(class)
trainingRowindex<-sample(1:nrow(completeData),0.78*nrow(completeData))
trainingData<-completeData[trainingRowindex,]
testData<-completeData[-trainingRowindex,]

TrainLabels<- trainingData[,10]
TestLabels<- testData[,10]
knnModel<- knn(trainingData[,-10],testData[,-10],cl=TrainLabels,k=10,prob=T)
library(gmodels)
CrossTable(testData[,10],knnModel,prop.chisq = F)
conf_knn1<-table(TestLabels,knnModel)
sum(diag(conf_knn1))/sum(conf_knn1)

```

